{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Technical-Debt-Large-Scale/qualification/blob/main/python/analysis/extractionatd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Technical-Debt-Large-Scale/qualification.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "file_xls_to_scan = 'qualification/xls/mergepatdsp.xls'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_my_xls_data = pd.read_excel(file_xls_to_scan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = df_my_xls_data\n",
    "df_bib_sort_by_year = df_data.copy()\n",
    "df_bib_sort_by_year['year'] = pd.to_numeric(df_bib_sort_by_year['year'])\n",
    "df_bib_sort_by_year = df_bib_sort_by_year.sort_values('year')\n",
    "\n",
    "df_data = df_bib_sort_by_year.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisa os conteudos dos Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove os abstracts vazios\n",
    "df_data = df_data[['key', 'year', 'list_authors', 'title', 'abstract']]\n",
    "df_data = df_data.dropna()\n",
    "df_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala a suite NLTK https://www.nltk.org/\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatena todos os abstracts em uma unica sentenca\n",
    "list_abstract = df_data.abstract.to_list()\n",
    "all_abstract_in_one = ' '.join(list_abstract)\n",
    "\n",
    "# Tokenization\n",
    "tokens_abstract = nltk.word_tokenize(all_abstract_in_one)\n",
    "tokens_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte todos os tokens de todos os abstracts em minusculo\n",
    "[item.lower() for item in tokens_abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recupera regras de frases em ingles\n",
    "tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')\n",
    "tokenizer.tokenize('Hello.  This is a test.  It works!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda em uma lista todas as frases que contem Architectural Technical Debt\n",
    "list_atd = []\n",
    "for abstract in list_abstract:\n",
    "  list_temp = tokenizer.tokenize(abstract)\n",
    "  for item in list_temp:\n",
    "    item_to_lower = item.lower()\n",
    "    atd_lower = 'Architectural Technical Debt'.lower()\n",
    "    if atd_lower in item_to_lower:\n",
    "      list_atd.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz o merge de todos as sentencas que contem ATD\n",
    "all_atd_abstract_in_one = ' '.join(list_atd)\n",
    "\n",
    "# Tokenization\n",
    "tokens_atd_abstract = nltk.word_tokenize(all_atd_abstract_in_one)\n",
    "\n",
    "# Lista de todos os tokens de todos os atd abstracts em minusculo\n",
    "list_of_tokens_atd_abstract = [item.lower() for item in tokens_atd_abstract]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz a contagem das palavras e guarda em um dicionario\n",
    "from collections import Counter\n",
    "\n",
    "counts_atd_words = Counter(list_of_tokens_atd_abstract)\n",
    "counts_atd_words\n",
    "\n",
    "dict_counts_atd_words = dict(counts_atd_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordena o dicionario de palavras por valor crescente\n",
    "# Mostra as palavras que mais se repetem\n",
    "sorted(dict_counts_atd_words.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove os stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all_atd_abstract_in_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "new_words = tokenizer.tokenize(all_atd_abstract_in_one)\n",
    "\n",
    "#word_tokens = word_tokenize(all_atd_abstract_in_one)\n",
    "word_tokens = new_words \n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "#filtered_sentence = []\n",
    " \n",
    "#for w in word_tokens:\n",
    "#    if w not in stop_words:\n",
    "#        filtered_sentence.append(w)\n",
    " \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de todos os tokens de todos os atd abstracts em minusculo sem o stop words\n",
    "list_of_tokens_atd_abstract_no_ponctuation = [item.lower() for item in filtered_sentence]\n",
    "\n",
    "# Faz a contagem das palavras e guarda em um dicionario\n",
    "from collections import Counter\n",
    "\n",
    "counts_atd_words_no_ponctuation = Counter(list_of_tokens_atd_abstract_no_ponctuation)\n",
    "\n",
    "dict_counts_atd_words_no_ponctuation = dict(counts_atd_words_no_ponctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordena o dicionario de palavras, sem os stop words, por valor crescente \n",
    "sorted(dict_counts_atd_words_no_ponctuation.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word *cloud*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!git clone https://github.com/amueller/word_cloud.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd word_cloud && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Word of Cloud about each file according frequence\n",
    "def generateWordCloud(name, counterWithFrequency, my_path='qualification/wordcloud'):\n",
    "  try:\n",
    "    wordcloud = WordCloud(width = 1200, height = 1000, random_state=1, background_color='black', colormap='Set2', collocations=False)\n",
    "    wordcloud.generate_from_frequencies(frequencies=counterWithFrequency)\n",
    "    # Display the generated image:\n",
    "    fileName = my_path + '/' + name + \".png\"\n",
    "    wordcloud.to_file(fileName)\n",
    "    print(f'Arquivo {fileName} gerado com sucesso!')\n",
    "  except Exception as ex:\n",
    "    print(f'Erro ao gerar o arquivo {name}')\n",
    "\n",
    "# Generate a Word of Cloud about text\n",
    "def generateWordCloud2(name, my_text, my_path='qualification/wordcloud'):\n",
    "  try:\n",
    "    wordcloud = WordCloud(width = 1200, height = 1000, random_state=1, background_color='black', colormap='Set2', collocations=False)\n",
    "    wordcloud.generate(my_text)\n",
    "    # Display the generated image:\n",
    "    fileName = my_path + '/' + name + \".png\"\n",
    "    wordcloud.to_file(fileName)\n",
    "    print(f'Arquivo {fileName} gerado com sucesso!')\n",
    "  except Exception as ex:\n",
    "    print(f'Erro ao gerar o arquivo {name}')\n",
    "\n",
    "# Generate a Word of Cloud about text\n",
    "def generateWordCloud3(name, my_text, my_path='qualification/wordcloud'):\n",
    "  try:\n",
    "    wordcloud = WordCloud(width=1200, height=1000).generate(my_text)\n",
    "    wordcloud.generate(my_text)\n",
    "    # Display the generated image:\n",
    "    fileName = my_path + '/' + name + \".png\"\n",
    "    wordcloud.to_file(fileName)\n",
    "    print(f'Arquivo {fileName} gerado com sucesso!')\n",
    "  except Exception as ex:\n",
    "    print(f'Erro ao gerar o arquivo {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateWordCloud('word_cloud_atd_no_ponctuation', dict_counts_atd_words_no_ponctuation)\n",
    "generateWordCloud2('word_cloud_atd_all_in_one', all_atd_abstract_in_one)\n",
    "generateWordCloud3('word_cloud_atd_all_in_one2', all_atd_abstract_in_one)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05ece30799c2dcdac4c13b3af20453da19de8df0d9a1de52cff7e0b6e1e82bdd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.3 64-bit ('3.6')",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
